# -*- coding: utf-8 -*-
"""CT_SRGAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_-GZyxnkvBVu893fNgG3k3mKl-COVkdQ

**1 - Download Data from Kaggle**
"""

! pip install kaggle
from google.colab import drive
drive.mount('/content/drive')
! mkdir ~/.kaggle
! cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json
! chmod 600 ~/.kaggle/kaggle.json
! kaggle datasets download -d nikhilroxtomar/ct-heart-segmentation
! unzip /content/ct-heart-segmentation.zip

"""**2- Super Resolution GAN**

***Libraries Calling***
"""

import pandas as pd
import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import ImageGrid
import shutil
import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, add, BatchNormalization, Activation, LeakyReLU, Layer
from tensorflow.keras.models import Model
from tensorflow.keras.applications import VGG19
from tensorflow.keras.utils import image_dataset_from_directory
from PIL import Image
from glob import glob
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from albumentations import HorizontalFlip, VerticalFlip, Rotate

"""**Listing Data**"""

import os
import glob
import cv2
import numpy as np

# Define the base directory
base_dir = "/content/data/train"

# Lists to store file paths
files_img = []
# Traverse through each folder in the base directory
for folder in os.listdir(base_dir):
    folder_path = os.path.join(base_dir, folder)

    # Ensure it's a directory
    if os.path.isdir(folder_path):
        images_path = os.path.join(folder_path, 'image')

        # Get all image and mask files
        files_img.extend(glob.glob(os.path.join(images_path, "*.png")))

# Verify the number of files
print(f"Found {len(files_img)} images.")

import shutil
os.makedirs("content/data/dir_datasethr", exist_ok=True)
dir_datasethr = "content/data/dir_datasethr"
for idx, file_path in enumerate(files_img, start=1):
    # Extracting file extension
    _, extension = os.path.splitext(file_path)
    new_file_name = f'image_{idx:04d}{extension}'
    shutil.copy(file_path, os.path.join(dir_datasethr, new_file_name))

"""Resize the images due to the cores"""

# Create the directory if it doesn't exist

os.makedirs("content/data/dir_dataset_hr", exist_ok=True)
dir_resized_hr = "content/data/dir_dataset_hr"
# Define the target size for resizing
target_size = (256, 256)  # Adjust size as needed

# Function to resize and save images
def resize_and_save(image_path, target_size, save_dir):
    # Load the image
    img = cv2.imread(image_path)
    if img is None:
        print(f"Error loading image: {image_path}")
        return

    # Resize the image
    resized_img = cv2.resize(img, target_size)

    # Extract the image filename
    filename = os.path.basename(image_path)

    # Save the resized image to the target directory
    save_path = os.path.join(save_dir, filename)
    cv2.imwrite(save_path, resized_img)
    print(f"Resized and saved: {save_path}")

# Loop through each image file in the directory
for filename in os.listdir(dir_datasethr):
    if filename.endswith(('.jpg', '.jpeg', '.png')):  # Check if the file is an image
        # Construct the full path to the image
        image_path = os.path.join(dir_datasethr, filename)

        # Resize and save the image
        resize_and_save(image_path, target_size, dir_resized_hr)

"""**Plot multiple images in a grid.**
 '''
    
    Parameters:
    - images (list): List of images to plot.
    - titles (list): List of titles for each image.
    - figsize (tuple): Figure size.
    - rows (int): Number of rows in the grid.
    - cols (int): Number of columns in the grid.
    - axes_pad (tuple): Padding between axes.

    Returns:
    - None
    '''
"""

def plot_images(images, titles, figsize, rows=1, cols=1, axes_pad=(0.1, 0.5)):
    # Create a new figure with the specified size
    fig = plt.figure(figsize=figsize)

    # Create an image grid within the figure.
    grid = ImageGrid(fig, 111, nrows_ncols=(rows, cols), axes_pad=axes_pad)

    # Iterate over the grid axes, the list of images, and the list of titles simultaneously
    for ax, img, title in zip(grid, images, titles):
        # Convert the image from BGR (OpenCV format) to RGB (Matplotlib format) and display it
        ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))

        # Set the title for the current subplot
        ax.set_title(title)

        # Turn off minor ticks on the axes
        ax.minorticks_off()

        # Remove x-axis tick labels
        ax.set_xticklabels([])

        # Remove y-axis tick labels
        ax.set_yticklabels([])

    # Display the figure with the plotted images
    plt.show()

"""**Downsample an image by a given scale.**

    Parameters:
    - img_file (str): Path to the image file.
    - scale (float): Scale factor to downsample the image.

    Returns:
    - img (numpy.ndarray): Original image.
    - img_small (numpy.ndarray): Downsampled image.

"""

def downsample(img_file, scale=0.3):
# Read the image from the specified file path
    img = cv2.imread(img_file, cv2.IMREAD_UNCHANGED)

    # Resize the image by the specified scale using nearest neighbor interpolation
    img_small = cv2.resize(img, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_NEAREST)

    # Return the original and downsampled images
    return img, img_small

# Downsample an example image for plotting
# Downsample the image at index 201 in the files_img list with a scale of 0.25
_, img_small = downsample(files_img[201], scale=0.25)

# Resize the downsampled image for comparison
# Read the original image from the file masks_path
img = cv2.imread(files_img[201], cv2.IMREAD_UNCHANGED)

# Resize the downsampled image to the size of the original image
img_small_resize = cv2.resize(img_small, (img.shape[1], img.shape[0]))

# Plot original and downsampled images
titles = ["original", "downsampled"]
images = [img, img_small_resize]

# Call the plot_images function to display the images
plot_images(images, titles, figsize=(10., 4.), cols=len(images))

# Directory to save downsampled images
dir_resized_hr = "/content/data/dir_resized_hr"

"""** Downsample and save all images in the dataset**"""

# Downsample and save all images in the dataset
os.makedirs("content/data/dir_dataset_lr", exist_ok=True)
dir_dataset_lr = "content/data/dir_dataset_lr"
for idx, file_img in enumerate(files_img, start=1):
    # Downsample each image in the files_img list
    img, img_small = downsample(file_img, scale=0.25)

    # Construct the path to save the downsampled image with a specific number
    file_img_lr = os.path.join(dir_dataset_lr, f'image_{idx:04d}.jpg')

    # Save the downsampled image to the specified path
    cv2.imwrite(file_img_lr, img_small)

# List all downsampled image files
files_img_lr = [os.path.join(dir_dataset_lr, x) for x in os.listdir(dir_dataset_lr)]

# Print the count of high-resolution (HR) and low-resolution (LR) images
print("HR images: {}, LR images: {}".format(len(files_img), len(files_img_lr)))

"""**GENERATOR**

    Build the generator model.

    Parameters:
    - input_shape (tuple): Shape of the input image.

    Returns:
    - model (tf.keras.Model): Generator model.

"""

def generator_model(input_shape):
    # Initialize variables
    g_init = tf.random_normal_initializer(1., 0.02)  # Initialize the gamma parameter for Batch Normalization layers
    relu = Activation('relu')  # Define ReLU activation function

    # Define input layer
    layer_input = Input(shape=input_shape)

    # First convolutional layer
    layer_past_block = Conv2D(64, (3, 3), padding='SAME', activation='relu', kernel_initializer='HeNormal')(layer_input)
    layer_initial = layer_past_block  # Store this layer for later use in residual connections

    # Residual blocks
    for i in range(16):
        layer = Conv2D(64, (3, 3), padding='SAME', kernel_initializer='HeNormal')(layer_past_block)
        layer = BatchNormalization(gamma_initializer=g_init)(layer)
        layer = add([layer_past_block, layer])  # Add skip connection
        layer_past_block = layer

    # Final residual connection
    layer = Conv2D(64, (3, 3), padding='SAME', kernel_initializer='HeNormal')(layer_past_block)
    layer = BatchNormalization(gamma_initializer=g_init)(layer)
    layer = add([layer, layer_initial])  # Add skip connection with the initial layer

    # Upsampling layers
    layer = Conv2D(256, (3, 3), padding='SAME', kernel_initializer='HeNormal')(layer)
    layer = SubpixelConv2D(upsampling_factor=2)(layer)  # Custom sub-pixel convolution layer for upsampling
    layer = relu(layer)

    layer = Conv2D(256, (3, 3), padding='SAME', kernel_initializer='HeNormal')(layer)
    layer = SubpixelConv2D(upsampling_factor=2)(layer)
    layer = relu(layer)

    # Final output layer
    layer_final = Conv2D(3, (1, 1), padding='SAME', kernel_initializer='HeNormal', activation='relu')(layer)

    # Define the model
    model = Model(inputs=layer_input, outputs=layer_final, name="generator")
    return model

class SubpixelConv2D(Layer):
    def __init__(self, upsampling_factor=2, **kwargs):
        """
        Custom layer for sub-pixel convolution.

        Parameters:
        - upsampling_factor (int): Factor by which to upsample.
        """
        super(SubpixelConv2D, self).__init__(**kwargs)
        self.upsampling_factor = upsampling_factor

    def build(self, input_shape):
        last_dim = input_shape[-1]
        factor = self.upsampling_factor * self.upsampling_factor
        if last_dim % factor != 0:
            raise ValueError('Channel ' + str(last_dim) + ' should be an integer multiple of upsampling_factor^2: ' + str(factor) + '.')

    def call(self, inputs, **kwargs):
        return tf.nn.depth_to_space(inputs, self.upsampling_factor)

    def get_config(self):
        config = {'upsampling_factor': self.upsampling_factor}
        base_config = super(SubpixelConv2D, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

    def compute_output_shape(self, input_shape):
        factor = self.upsampling_factor * self.upsampling_factor
        input_shape_1 = input_shape[1] * self.upsampling_factor if input_shape[1] is not None else None
        input_shape_2 = input_shape[2] * self.upsampling_factor if input_shape[2] is not None else None
        dims = [input_shape[0], input_shape_1, input_shape_2, int(input_shape[3] / factor)]
        return tuple(dims)

"""**Discriminator**
 Build the discriminator model.

    Parameters:
    - input_shape (tuple): Shape of the input image.

    Returns:
    - model (tf.keras.Model): Discriminator model.
"""

def discriminator_model(input_shape):
    # Initialize variables
    g_init = tf.random_normal_initializer(1., 0.02)  # Initialize the gamma parameter for Batch Normalization layers
    ly_relu = LeakyReLU(alpha=0.2)  # Define Leaky ReLU activation function
    df_dim = 16  # Dimension factor for the discriminator

    # Define input layer
    layer_input = Input(input_shape)

    # First convolutional layer
    layer = Conv2D(64, (4, 4), (2, 2), padding='SAME', kernel_initializer='HeNormal')(layer_input)
    layer = ly_relu(layer)

    # Convolutional layers with increasing filters
    for i in range(2, 6):
        layer = Conv2D(df_dim * (2 ** i), (4, 4), (2, 2), padding='SAME', kernel_initializer='HeNormal')(layer)
        layer = ly_relu(layer)
        layer = BatchNormalization(gamma_initializer=g_init)(layer)

    # Additional convolutional layers
    layer = Conv2D(df_dim * 16, (1, 1), (1, 1), padding='SAME', kernel_initializer='HeNormal')(layer)
    layer = ly_relu(layer)
    layer = BatchNormalization(gamma_initializer=g_init)(layer)

    layer = Conv2D(df_dim * 8, (1, 1), (1, 1), padding='SAME', kernel_initializer='HeNormal')(layer)
    layer = BatchNormalization(gamma_initializer=g_init)(layer)
    layer_past = layer  # Store this layer for the residual block

    # Residual Block
    layer = Conv2D(df_dim * 4, (3, 3), (1, 1), padding='SAME', kernel_initializer='HeNormal')(layer)
    layer = ly_relu(layer)
    layer = Conv2D(df_dim * 8, (3, 3), (1, 1), padding='SAME', kernel_initializer='HeNormal')(layer)
    layer = BatchNormalization(gamma_initializer=g_init)(layer)
    layer = add([layer_past, layer])  # Add skip connection
    layer = ly_relu(layer)

    # Flatten the output and add dense layers
    layer = Flatten()(layer)
    layer = Dense(1024)(layer)
    layer = ly_relu(layer)
    layer_final = Dense(1, activation='sigmoid')(layer)  # Final output layer with sigmoid activation

    # Define the model
    model = Model(inputs=layer_input, outputs=layer_final, name="discriminator")
    return model

"""Loss functions"""

def adversarial_loss(y_true, y_pred):
    """
    Compute adversarial loss.

    Parameters:
    - y_true (tensor): Ground truth labels.
    - y_pred (tensor): Predicted labels.

    Returns:
    - loss (tensor): Adversarial loss.
    """
    return tf.keras.losses.binary_crossentropy(y_true, y_pred)

def content_loss(y_true, y_pred):
    """
    Compute content loss using VGG19.

    Parameters:
    - y_true (tensor): Ground truth images.
    - y_pred (tensor): Generated images.

    Returns:
    - loss (tensor): Content loss.
    """
    y_true = tf.keras.applications.vgg19.preprocess_input(y_true)
    y_pred = tf.keras.applications.vgg19.preprocess_input(y_pred)

    vgg19 = VGG19(include_top=False, input_shape=(96, 96, 3))
    loss_model = Model(vgg19.input, vgg19.get_layer("block5_conv4").output)
    loss_model.trainable = False
    return tf.keras.losses.mean_squared_error(loss_model(y_true), loss_model(y_pred))

def srgan_loss(y_true, y_pred):
    """
    Compute SRGAN loss, a combination of adversarial and content losses.

    Parameters:
    - y_true (tensor): Ground truth images.
    - y_pred (tensor): Generated images.

    Returns:
    - loss (tensor): SRGAN loss.
    """
    return content_loss(y_true, y_pred) + 1e-3 * adversarial_loss(y_true, y_pred)

# Define image size and batch size
size_image = (256, 256)
batch_size = 4

files_img[1:10]

train_hr_dataset = image_dataset_from_directory(
    dir_resized_hr,
    label_mode=None,
    image_size=size_image,
    batch_size=batch_size,
    interpolation='bilinear'
)

train_lr_dataset = image_dataset_from_directory(
    dir_dataset_lr,
    label_mode=None,
    image_size=size_image,
    batch_size=batch_size,
    interpolation='bilinear'
)

# Input shape for generator and discriminator models
input_shape_gen = (256, 256, 3)
input_shape_dis = (256, 256, 3)

# Instantiate generator and discriminator models
generator = generator_model(input_shape_gen)
discriminator = discriminator_model(input_shape_dis)

# Compile generator and discriminator models
generator.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999), loss=srgan_loss)
discriminator.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999), loss=adversarial_loss)

def train_step(lr_images, hr_images):
    """
    Perform a single training step.

    Parameters:
    - lr_images (tensor): Low-resolution images.
    - hr_images (tensor): High-resolution images.

    Returns:
    - gen_loss (tensor): Generator loss.
    - disc_loss (tensor): Discriminator loss.
    """
    # Open a gradient tape to record operations for automatic differentiation
    with tf.GradientTape() as tape:
        # Generate super-resolution images from low-resolution input images
        sr_images = generator(lr_images, training=True)

        # Pass high-resolution images through the discriminator
        real_output = discriminator(hr_images, training=True)

        # Pass generated super-resolution images through the discriminator
        fake_output = discriminator(sr_images, training=True)

        # Compute the generator loss using content loss and adversarial loss
        gen_loss = srgan_loss(hr_images, sr_images)

        # Compute the discriminator loss by comparing real and fake outputs
        disc_loss = adversarial_loss(tf.ones_like(real_output), real_output) + \
                    adversarial_loss(tf.zeros_like(fake_output), fake_output)

    # Compute gradients of generator and discriminator with respect to their trainable variables
    gradients_of_generator = tape.gradient(gen_loss, generator.trainable_variables)
    gradients_of_discriminator = tape.gradient(disc_loss, discriminator.trainable_variables)

    # Apply gradients to update generator and discriminator weights
    generator.optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator.optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))

    return gen_loss, disc_loss

def train(dataset, epochs):
    """
    Train the SRGAN model.

    Parameters:
    - dataset (tf.data.Dataset): Training dataset.
    - epochs (int): Number of epochs to train.

    Returns:
    - None
    """
    # Loop through each epoch
    for epoch in range(epochs):
        # Iterate over each batch in the dataset
        for lr_images, hr_images in dataset:
            # Perform a single training step
            gen_loss, disc_loss = train_step(lr_images, hr_images)

        # Print generator and discriminator losses at the end of each epoch
        print("Epoch: {}, Generator Loss: {}, Discriminator Loss: {}".format(epoch, gen_loss, disc_loss))

# Combine HR and LR datasets into a single dataset
dataset = tf.data.Dataset.zip((train_lr_dataset, train_hr_dataset))

# Train the model for 100 epochs
train(dataset, epochs=100)